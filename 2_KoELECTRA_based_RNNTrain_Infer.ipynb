{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds\n",
    "import random\n",
    "\n",
    "# os\n",
    "import glob, os, sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#################################################################\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_info_columns', 300)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# print\n",
    "from pprint import pprint\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['xtick.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "#################################################################\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertPreTrainedModel, BertModel, Trainer, TrainingArguments\n",
    "\n",
    "# others\n",
    "from itertools import chain\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import copy\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/news_train.csv')\n",
    "test_df = pd.read_csv('./data/news_test.csv')\n",
    "train_df = train_df.rename(columns = {'info' : 'info_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"monologg/koelectra-base-v3-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "print(f'[CLS] : {tokenizer.get_vocab()[\"[CLS]\"]}')\n",
    "print(f'[SEP] : {tokenizer.get_vocab()[\"[SEP]\"]}')\n",
    "print(f'[PAD] : {tokenizer.get_vocab()[\"[PAD]\"]}')\n",
    "print(f'[UNK] : {tokenizer.get_vocab()[\"[UNK]\"]}')\n",
    "\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['[EOP]']})\n",
    "\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스단위 집계, 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_news(df) :\n",
    "    news = []\n",
    "    title = df.title.iloc[0]\n",
    "    news.append(title)\n",
    "    body_list_of_sent = list(df.content)\n",
    "    news.extend(body_list_of_sent)\n",
    "    \n",
    "    res = pd.Series({'news' : news})\n",
    "    return res\n",
    "\n",
    "test_news = test_df.groupby('n_id').apply(sent_to_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>(토큰 제한하고) ids로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_to_ids(l) :\n",
    "    res = [2]\n",
    "    for sent in l :\n",
    "        res += tokenizer.encode(sent)[:20]\n",
    "        res += [35000] # 뉴스 한 문장이 끝날 때마다 [EOP] 토큰을 추가\n",
    "    res.pop() # 마지막 [EOP] 토큰 대신 [SEP]를 넣어서 뉴스 종료를 알림\n",
    "    res += [3]\n",
    "    \n",
    "    return res\n",
    "\n",
    "test_news['tokens_ids'] = test_news.news.apply(news_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 길이가 제일 긴 뉴스는 6741개...\n",
    "test_news.tokens_ids.apply(lambda x : len(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOP_ID = tokenizer.get_vocab()[\"[EOP]\"]\n",
    "SEP_ID = tokenizer.get_vocab()[\"[SEP]\"]\n",
    "\n",
    "def ids_to_segments_ids(ids) :\n",
    "    _segs = [-1] + [i for i, id_ in enumerate(ids) if id_ == EOP_ID or id_ == SEP_ID]\n",
    "    segs = [_segs[i] - _segs[i-1] for i in range(1, len(_segs))]\n",
    "    segments_ids = []\n",
    "    \n",
    "    for i, s in enumerate(segs) :\n",
    "        if (i % 2 == 0) :\n",
    "            segments_ids += s * [0]\n",
    "        else :\n",
    "            segments_ids += s * [1]\n",
    "    return segments_ids\n",
    "\n",
    "test_news['segments_ids'] = test_news.tokens_ids.apply(ids_to_segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news['masks'] = test_news.tokens_ids.apply(lambda x : [1] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eop 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_ID = tokenizer.get_vocab()[\"[EOP]\"]\n",
    "\n",
    "def ids_to_cls_idxs(l):\n",
    "    cls_idxs = [i for i, t in enumerate(l) if t == CLS_ID]\n",
    "    return cls_idxs\n",
    "\n",
    "test_news['cls_idxs'] = test_news.tokens_ids.apply(ids_to_cls_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.cls_idxs.apply(lambda x : len(x)).describe() # eop 1개인 경우도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_news.tokens_ids.apply(lambda x : len(x)) - test_news.masks.apply(lambda x : len(x))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# divide news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for electra+rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news = test_news[test_news.tokens_ids.apply(lambda x : len(x)) <= 512].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_test_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news.tokens_ids.apply(lambda x : len(x)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for electra+nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = test_news[test_news.tokens_ids.apply(lambda x : len(x)) > 512].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news_2 = test_df[test_df.n_id.isin(n_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer electra_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문 진행중 진행상황 프린트 함수\n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval\n",
    "\n",
    "# 초->시 변환\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "class CustomBERTModel(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(MODEL)\n",
    "        self.bert = AutoModel.from_pretrained(MODEL, config = self.config) # 수정 필요\n",
    "        self.bert.resize_token_embeddings(35001)\n",
    "        \n",
    "    def forward(self, ids, segs, mask) :\n",
    "        output = self.bert(input_ids = ids, token_type_ids = segs, attention_mask = mask)\n",
    "        \n",
    "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        sequence_output = output[0]\n",
    "#         pooled_output = output[1]\n",
    "\n",
    "        return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LayerNormLSTMCell(nn.LSTMCell):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__(input_size, hidden_size, bias)\n",
    "\n",
    "        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n",
    "        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n",
    "        self.ln_ho = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        self.check_forward_input(input)\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "        self.check_forward_hidden(input, hx, '[0]')\n",
    "        self.check_forward_hidden(input, cx, '[1]')\n",
    "\n",
    "        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n",
    "                + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n",
    "        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n",
    "        g = gates[:, (3 * self.hidden_size):].tanh()\n",
    "\n",
    "        cy = (f * cx) + (i * g)\n",
    "        hy = o * self.ln_ho(cy).tanh()\n",
    "        return hy, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.hidden0 = nn.ModuleList([\n",
    "            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                              hidden_size=hidden_size, bias=bias)\n",
    "            for layer in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.hidden1 = nn.ModuleList([\n",
    "                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                                  hidden_size=hidden_size, bias=bias)\n",
    "                for layer in range(num_layers)\n",
    "            ])\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "\n",
    "        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
    "        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
    "\n",
    "        if self.bidirectional:\n",
    "            xs = input\n",
    "            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n",
    "                l0, l1 = 2 * l, 2 * l + 1\n",
    "                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n",
    "                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n",
    "                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n",
    "                    h0, c0 = ht[t][l0], ct[t][l0]\n",
    "                    t = seq_len - 1 - t\n",
    "                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n",
    "                    h1, c1 = ht[t][l1], ct[t][l1]\n",
    "                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n",
    "            y = torch.stack(xs)\n",
    "            hy = torch.stack(ht[-1])\n",
    "            cy = torch.stack(ct[-1])\n",
    "        else:\n",
    "            h, c = hx, cx\n",
    "            for t, x in enumerate(input):\n",
    "                for l, layer in enumerate(self.hidden0):\n",
    "                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n",
    "                    x = ht[t][l]\n",
    "                h, c = ht[t], ct[t]\n",
    "            y = torch.stack([h[-1] for h in ht])\n",
    "            hy = torch.stack(ht[-1])\n",
    "            cy = torch.stack(ct[-1])\n",
    "\n",
    "        return y, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, bidirectional, num_layers, input_size,\n",
    "                 hidden_size, dropout=0.0):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.rnn = LayerNormLSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional)\n",
    "\n",
    "        self.wo = nn.Linear(num_directions * hidden_size, 1, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        memory_bank, _ = self.rnn(x)\n",
    "        memory_bank = self.dropout(memory_bank) + x\n",
    "        memory_bank = torch.transpose(memory_bank, 1, 0)\n",
    "\n",
    "        sent_scores = self.sigmoid(self.wo(memory_bank))\n",
    "        sent_scores = sent_scores.squeeze(-1)\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.bert = CustomBERTModel()\n",
    "#         self.encoder = Classifier()\n",
    "        self.encoder = RNNEncoder(bidirectional=True, num_layers=1,\n",
    "                                      input_size=self.bert.config.hidden_size, hidden_size=768, # args.rnn_size\n",
    "                                      dropout=0.1) # args.dropout\n",
    "        for p in self.encoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                \n",
    "    def forward(self, x, segs, mask, clss):\n",
    "        top_vec = self.bert(x, segs, mask) # (b, len, hidden)\n",
    "#         sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss] # [b, ]\n",
    "        sents_vec = top_vec[:, clss.squeeze(0), :]  # [b, eop len, hidden]\n",
    "        sent_scores = self.encoder(sents_vec) # [b, eop len]\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model:\n",
    "model_name = '1231_06:17_20_0.9987.pth'\n",
    "\n",
    "state = torch.load(os.path.join('./models/morenews_electra_rnn_without_cleaning/'+model_name))\n",
    "model.load_state_dict(state['model'])\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "device = torch.device('cuda:3')\n",
    "print('  GPU:', torch.cuda.get_device_name())\n",
    "desc = model.to(device)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_inputs_t = list(map(lambda x : torch.tensor(x).unsqueeze(0), list(sub_test_news.tokens_ids)))\n",
    "pt_token_type_ids_t = list(map(lambda x : torch.tensor(x).unsqueeze(0), list(sub_test_news.segments_ids)))\n",
    "pt_clss_t = list(map(lambda x : torch.tensor(x).unsqueeze(0), list(sub_test_news.cls_idxs)))\n",
    "pt_masks_t = list(map(lambda x : torch.tensor(x).unsqueeze(0), list(sub_test_news.masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# 스마트 배치\n",
    "\n",
    "# Choose an interval on which to print progress updates.\n",
    "update_interval_eval = good_update_interval(total_iters=len(pt_inputs_t), \n",
    "                                       num_desired_updates=10)\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# For each batch of training data...\n",
    "for step in range(0, len(pt_inputs_t)):\n",
    "\n",
    "    # Progress update every 100 batches.\n",
    "    if step % update_interval_eval == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "\n",
    "        # Calculate the time remaining based on our progress.\n",
    "        steps_per_sec = (time.time() - t0) / step\n",
    "        remaining_sec = steps_per_sec * (len(pt_inputs_t) - step)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(pt_inputs_t), elapsed, remaining))\n",
    "\n",
    "    # Copy the batch to the GPU.\n",
    "    b_inputs = pt_inputs_t[step].to(device)\n",
    "    b_token_type_ids = pt_token_type_ids_t[step].to(device)\n",
    "    b_masks = pt_masks_t[step].to(device)\n",
    "    b_clss = pt_clss_t[step].to(device)\n",
    "\n",
    "#     b_labels = pt_labels_t[step].to(device)\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_inputs, b_token_type_ids, b_masks, b_clss)\n",
    "\n",
    "#     loss = criterion(outputs, b_labels)\n",
    "\n",
    "#     total_val_loss += loss.sum().item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs.squeeze(0).detach().cpu().numpy()\n",
    "#     label_ids = b_labels.squeeze(0).to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "#     true_labels.append(label_ids)\n",
    "\n",
    "# Calculate the average val loss over all of the batches.\n",
    "# avg_val_loss = total_val_loss / len(pt_inputs_t)\n",
    "# val_loss.append(avg_val_loss)\n",
    "\n",
    "# Combine the results across the batches.\n",
    "predictions_flatten = np.concatenate(predictions, axis=0)\n",
    "# true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = []\n",
    "info_ = []\n",
    "for n_id, n_predictions in zip(sub_test_news.index, predictions) :\n",
    "    n_preds = np.round(n_predictions)\n",
    "    for i, pred in enumerate(n_preds) :\n",
    "        id_.append(n_id + '_' + str(i+1))\n",
    "        info_.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_infer = pd.DataFrame({'id' : id_, 'info' : info_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer electra+nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-Trained Model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the Config object, with an output configured for classification.\n",
    "config = AutoConfig.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=2)\n",
    "# config = AutoConfig.from_pretrained(\"beomi/kcbert-base\", num_labels=2)\n",
    "\n",
    "print('Config type:', str(type(config)), '\\n')\n",
    "\n",
    "# pre-trained kcbert 로드\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for classification, passing in the `config` from above.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n",
    "    config=config)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     pretrained_model_name_or_path=\"beomi/kcbert-base\",\n",
    "#     config=config)\n",
    "\n",
    "print('\\nModel type:', str(type(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 토큰 길이 설정\n",
    "max_len = 300\n",
    "\n",
    "# 배치 크기 지정\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model:\n",
    "model_name = '1211_18_26_4_0.9977.pth'\n",
    "\n",
    "state = torch.load(os.path.join('./models/koelectra/'+model_name))\n",
    "model.load_state_dict(state['model'])\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "device = torch.device('cuda:3')\n",
    "print('  GPU:', torch.cuda.get_device_name())\n",
    "desc = model.to(device)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# 배치 생성 함수\n",
    "def make_smart_batches_pred(title_samples, content_samples, batch_size):\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(title_samples), batch_size))\n",
    "\n",
    "    # ==============================\n",
    "    #    토큰화 & Truncate(패딩없이)\n",
    "    # ==============================\n",
    "\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(title_samples)))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(title_samples), num_desired_updates=10)\n",
    "    \n",
    "    text_samples = list(zip(title_samples, content_samples))\n",
    "\n",
    "    # 모든 문장에 대해\n",
    "    for title, content in text_samples:\n",
    "        \n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # padding 없이 토큰화\n",
    "        input_ids = tokenizer.encode(title, content,              # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "        \n",
    "                                    \n",
    "        # full_input_ids\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================\n",
    "    \n",
    "    # test cases의 입력 순서를 유지하기 위해, sort하지 않는다\n",
    "    samples = list(full_input_ids)\n",
    "\n",
    "    print('{:>10,} samples without sorting for prediction\\n'.format(len(samples)))\n",
    "\n",
    "    # 각 배치 담을 리스트\n",
    "    batch_ordered_sentences = []\n",
    "    \n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "\n",
    "    # 모든 샘플을 배치화할 때까지..\n",
    "    while len(samples) > 0:\n",
    "\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # 인덱스는 순서대로 선택\n",
    "        select = 0\n",
    "\n",
    "        # 배치\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        # 배치 토큰\n",
    "        batch_ordered_sentences.append([s for s in batch])\n",
    "\n",
    "        # 배치를 샘플에서 제거\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "\n",
    "    # (비슷한 토큰 길이를 가지는) 각 배치마다 패딩 추가된 인풋 생성\n",
    "    for batch_inputs in batch_ordered_sentences:\n",
    "\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # 배치 내에서 가장 긴 문장\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # 각 문장에 대해\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # 추가할 패딩 개수\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # 패딩 추가\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # 어텐션 마스크\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # 개별 배치의 결과\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # 각 배치의 인풋 생성 결과를 저장\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "    \n",
    "    print('  DONE.')\n",
    "    \n",
    "    # 모델의 최종 인풋\n",
    "    return (py_inputs, py_attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = sub_test_news_2['title'].values\n",
    "test_content = sub_test_news_2['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 테스트셋 배치 생성\n",
    "(py_inputs, py_attn_masks) = make_smart_batches_pred(test_title, test_content, batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_title)))\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Choose an interval on which to print progress updates.\n",
    "update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# Put model in prediction mode\n",
    "model.eval()\n",
    "\n",
    "# For each batch of training data...\n",
    "for step in range(0, len(py_inputs)):\n",
    "\n",
    "    # Progress update every 100 batches.\n",
    "    if step % update_interval == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        steps_per_sec = (time.time() - t0) / step\n",
    "        remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "    # Copy the batch to the GPU.\n",
    "    b_input_ids = py_inputs[step].to(device)\n",
    "    b_input_mask = py_attn_masks[step].to(device)\n",
    "  \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Choose the label with the highest score as our prediction.\n",
    "preds = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_news_2['info'] = preds\n",
    "electra_infer_2 = sub_test_news_2[['id', 'info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_infer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = pd.concat([electra_infer, electra_infer_2])\n",
    "infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.set_index('id').loc[test_df['id']].reset_index().to_csv('./submission/cut_token_no_cleaned_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
