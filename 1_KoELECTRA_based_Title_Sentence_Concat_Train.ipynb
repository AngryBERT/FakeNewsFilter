{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, sys\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.max_info_columns', 250)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "mpl.rcParams['axes.titlesize'] = 15\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['xtick.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "# others\n",
    "from itertools import chain\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta, datetime\n",
    "import copy\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# others2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertPreTrainedModel, BertModel, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/news_train.csv')\n",
    "test = pd.read_csv('./data/news_test.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "# Randomly choose some examples.\n",
    "for i in range(4):\n",
    "    \n",
    "    # Choose a random sample by index.\n",
    "    j = random.choice(range(len(train)))\n",
    "    \n",
    "    # Print out the label and the text. \n",
    "    print('==== Label: {:} ===='.format(train['info'][j]))\n",
    "    print('==== Ord: {:} ===='.format(train['ord'][j]))\n",
    "    print(wrapper.fill(' | '.join([train['title'][j], train['content'][j]])))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "# pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x):\n",
    "#     x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    \n",
    "    x.replace('&nbsp;', ' ')\n",
    "    x.replace('&quot;', '\\\"')\n",
    "    x.replace('&ldquo;', '\\\"')\n",
    "    x.replace('&rdquo;', '\\\"')\n",
    "    x.replace('&#039;', '\\'')\n",
    "    x.replace('&#39;', '\\'')\n",
    "    x.replace('&#035;', '#')\n",
    "    x.replace('&#35;', '#')\n",
    "    x.replace('&apos;', '\\'')\n",
    "    x.replace('&amp;', '&')\n",
    "    x.replace('&lt;', '<')\n",
    "    x.replace('&gt;', '>')\n",
    "    x.replace('000님', '익명')\n",
    "\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'] = train['title'].apply(clean)\n",
    "test['title'] = test['title'].apply(clean)\n",
    "\n",
    "train['content'] = train['content'].apply(clean)\n",
    "test['content'] = test['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this whole process & results reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev Split for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title = train['title'].values\n",
    "train_content = train['content'].values\n",
    "\n",
    "train_labels = train['info'].values\n",
    "\n",
    "test_title = test['title'].values\n",
    "test_content = test['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title, X_test_title, X_train_content, X_test_content, y_train, y_test = train_test_split(train_title, \n",
    "                                                                                                 train_content, train_labels, \n",
    "                                                    test_size=.1, \n",
    "                                                    random_state=seed_val, \n",
    "                                                    stratify=train_labels)\n",
    "\n",
    "# 길이 확인\n",
    "print(len(X_train_title), len(X_test_title))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문 진행중 진행상황 프린트 함수\n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval\n",
    "\n",
    "# 초->시 변환\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Pre-Trained Model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the Config object, with an output configured for classification.\n",
    "config = AutoConfig.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=2)\n",
    "print('Config type:', str(type(config)), '\\n')\n",
    "\n",
    "# pre-trained model 로드\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for classification, passing in the `config` from above.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n",
    "    config=config)\n",
    "\n",
    "print('\\nModel type:', str(type(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 토큰 길이 설정\n",
    "max_len = 300\n",
    "\n",
    "# 배치 크기 지정\n",
    "batch_size = 16\n",
    "\n",
    "# epoch 지정\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print('  GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
    "\n",
    "# model = nn.DataParallel(model, output_device=1)\n",
    "\n",
    "desc = model.to(device)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # 1e-5, 2e-5, 3e-5, 5e-5\n",
    "                  eps = 1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "batches = len(X_train_title)//batch_size + 1\n",
    "total_steps = batches * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = batches, # default 0\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Batch for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 생성 함수\n",
    "def make_smart_batches(title_samples, content_samples, labels, batch_size):\n",
    "    \n",
    "    '''\n",
    "    문장을 padding없이 토큰화하고 - 토큰화 된 길이가 비슷한 문장끼리 배치 생성 - padding 추가된 인풋 생성\n",
    "    '''\n",
    "\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(title_samples), batch_size))\n",
    "\n",
    "    # ==============================\n",
    "    #   토큰화 & Truncate(패딩없이)\n",
    "    # ==============================\n",
    "\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
    "    \n",
    "    text_samples = list(zip(title_samples, content_samples))\n",
    "\n",
    "    # 모든 문장에 대해\n",
    "    for title, content in text_samples:\n",
    "        \n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # padding 없이 토큰화\n",
    "        input_ids = tokenizer.encode(title, content,               # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "                                    \n",
    "        # full_input_ids\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================\n",
    "    \n",
    "    # 토큰 길이에 따라 정렬\n",
    "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
    "\n",
    "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
    "\n",
    "    # 각 배치 담을 리스트\n",
    "    batch_ordered_sentences = []\n",
    "    batch_ordered_labels = []\n",
    "\n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "\n",
    "    # 모든 샘플을 배치화할 때까지..\n",
    "    while len(samples) > 0:\n",
    "\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # 랜덤 인덱스 선택\n",
    "        select = random.randint(0, len(samples) - to_take)\n",
    "\n",
    "        # 배치\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        # 배치 토큰\n",
    "        batch_ordered_sentences.append([s[0] for s in batch])\n",
    "        # 배치 라벨\n",
    "        batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "        # 배치를 샘플에서 제거\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "    py_labels = []\n",
    "\n",
    "    # (비슷한 토큰 길이를 가지는) 각 배치마다 패딩 추가된 인풋 생성\n",
    "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # 배치 내에서 가장 긴 문장\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # 각 문장에 대해\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # 추가할 패딩\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # 패딩 추가\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # 어텐션 마스크\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # 개별 배치의 결과\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # 각 배치의 인풋 생성 결과를 저장\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "        py_labels.append(torch.tensor(batch_labels))\n",
    "    \n",
    "    print('  DONE.')\n",
    "    \n",
    "    # 모델의 최종 인풋\n",
    "    return (py_inputs, py_attn_masks, py_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We'll store a number of quantities such as training and val_loss, val_acc, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Update every `update_interval` batches.\n",
    "update_interval = good_update_interval(total_iters=batches, num_desired_updates=5)\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# assign 'score' to save best model only\n",
    "best_score = 0\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# to visualize loss per each epoch\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    \n",
    "    # At the start of each epoch (except for the first) we need to re-randomize our training data.\n",
    "    # Use our `make_smart_batches` function to re-shuffle the dataset into new batches.\n",
    "    (py_inputs, py_attn_masks, py_labels) = make_smart_batches(X_train_title, X_train_content, y_train, batch_size)\n",
    "    \n",
    "    print('Training on {:,} batches...'.format(len(py_inputs)))\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(0, len(py_inputs)):\n",
    "\n",
    "        # Progress update every, e.g., 100 batches.\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Calculate the time remaining based on our progress.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "            remaining = format_time(remaining_sec)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "        # Copy the current training batch to the GPU using the `to` method.\n",
    "        b_input_ids = py_inputs[step].to(device)\n",
    "        b_input_mask = py_attn_masks[step].to(device)\n",
    "        b_labels = py_labels[step].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The call returns the loss (because we provided labels) and the \"logits\"--the model outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n",
    "        # `loss` is a Tensor containing a single value; \n",
    "        # the `.item()` function just returns the Python value from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0, to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"\n",
    "        # how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(py_inputs)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    #               Evaluation\n",
    "    # ========================================\n",
    "\n",
    "    if epoch_i % 2 == 1: # control denominator(1~5 recommended)\n",
    "        print('Predicting labels for {:,} test sentences...'.format(len(y_test)))\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        # 스마트 배치\n",
    "        (py_inputs, py_attn_masks, py_labels) = make_smart_batches(X_test_title, X_test_content, y_test, batch_size)\n",
    "\n",
    "        # Choose an interval on which to print progress updates.\n",
    "        update_interval_eval = good_update_interval(total_iters=len(py_inputs), \n",
    "                                               num_desired_updates=10)\n",
    "\n",
    "        # Measure elapsed time.\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_val_loss = 0\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step in range(0, len(py_inputs)):\n",
    "\n",
    "            # Progress update every 100 batches.\n",
    "            if step % update_interval_eval == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Calculate the time remaining based on our progress.\n",
    "                steps_per_sec = (time.time() - t0) / step\n",
    "                remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "                remaining = format_time(remaining_sec)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "            # Copy the batch to the GPU.\n",
    "            b_input_ids = py_inputs[step].to(device)\n",
    "            b_input_mask = py_attn_masks[step].to(device)\n",
    "            b_labels = py_labels[step].to(device)\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                loss, logits = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask,\n",
    "                               labels = b_labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "            \n",
    "        # Calculate the average val loss over all of the batches.\n",
    "        avg_val_loss = total_val_loss / len(py_inputs)\n",
    "        val_loss.append(avg_val_loss)\n",
    "\n",
    "        # Combine the results across the batches.\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "        # Choose the label with the highest score as our prediction.\n",
    "        preds = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "        print(classification_report(true_labels, preds))\n",
    "\n",
    "        acc = accuracy_score(true_labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n",
    "        \n",
    "        print('accuracy', acc)\n",
    "        print('f1(weighted)',  f1)\n",
    "        print('precision', precision)\n",
    "        print('recall', recall)\n",
    "        print(\"\")\n",
    "        print(\"Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
    "    \n",
    "        if acc > best_score:\n",
    "            best_score = acc\n",
    "\n",
    "            model_dir = './models'\n",
    "            state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "\n",
    "            now = time.strftime('%m%d_%H:%M')\n",
    "            torch.save(state, os.path.join(model_dir, '_'.join([now, \n",
    "                                                                str(epoch_i+1), \n",
    "                                                                str(round(acc,4))]) + '.pth'))\n",
    "            print('model saved')\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,'g*', val_loss, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Batch for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 생성 함수\n",
    "def make_smart_batches_pred(title_samples, content_samples, batch_size):\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(title_samples), batch_size))\n",
    "\n",
    "    # ==============================\n",
    "    #    토큰화 & Truncate(패딩없이)\n",
    "    # ==============================\n",
    "\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(title_samples)))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(title_samples), num_desired_updates=10)\n",
    "    \n",
    "    text_samples = list(zip(title_samples, content_samples))\n",
    "\n",
    "    # 모든 문장에 대해\n",
    "    for title, content in text_samples:\n",
    "        \n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # padding 없이 토큰화\n",
    "        input_ids = tokenizer.encode(title, content,              # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "        \n",
    "                                    \n",
    "        # full_input_ids\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================\n",
    "    \n",
    "    # test cases의 입력 순서를 유지하기 위해, sort하지 않는다\n",
    "    samples = list(full_input_ids)\n",
    "\n",
    "    print('{:>10,} samples without sorting for prediction\\n'.format(len(samples)))\n",
    "\n",
    "    # 각 배치 담을 리스트\n",
    "    batch_ordered_sentences = []\n",
    "    \n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "\n",
    "    # 모든 샘플을 배치화할 때까지..\n",
    "    while len(samples) > 0:\n",
    "\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # 인덱스는 순서대로 선택\n",
    "        select = 0\n",
    "\n",
    "        # 배치\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        # 배치 토큰\n",
    "        batch_ordered_sentences.append([s for s in batch])\n",
    "\n",
    "        # 배치를 샘플에서 제거\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "\n",
    "    # (비슷한 토큰 길이를 가지는) 각 배치마다 패딩 추가된 인풋 생성\n",
    "    for batch_inputs in batch_ordered_sentences:\n",
    "\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # 배치 내에서 가장 긴 문장\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # 각 문장에 대해\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # 추가할 패딩 개수\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # 패딩 추가\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # 어텐션 마스크\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # 개별 배치의 결과\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # 각 배치의 인풋 생성 결과를 저장\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "    \n",
    "    print('  DONE.')\n",
    "    \n",
    "    # 모델의 최종 인풋\n",
    "    return (py_inputs, py_attn_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model:\n",
    "model_name = '1211_18:26_4_0.9977.pth'\n",
    "\n",
    "state = torch.load(os.path.join('./models/'+model_name))\n",
    "model.load_state_dict(state['model'])\n",
    "if optimizer is not None:\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "if scheduler is not None:\n",
    "    scheduler.load_state_dict(state['scheduler'])\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for unlabeled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 배치 생성\n",
    "(py_inputs, py_attn_masks) = make_smart_batches_pred(test_title, test_content, batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_title)))\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Choose an interval on which to print progress updates.\n",
    "update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# Put model in prediction mode\n",
    "model.eval()\n",
    "\n",
    "# For each batch of training data...\n",
    "for step in range(0, len(py_inputs)):\n",
    "\n",
    "    # Progress update every 100 batches.\n",
    "    if step % update_interval == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        steps_per_sec = (time.time() - t0) / step\n",
    "        remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "    # Copy the batch to the GPU.\n",
    "    b_input_ids = py_inputs[step].to(device)\n",
    "    b_input_mask = py_attn_masks[step].to(device)\n",
    "  \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Choose the label with the highest score as our prediction.\n",
    "preds = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strings = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(j)) for i in py_inputs for j in np.array(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "check_inference = pd.DataFrame({'content' : input_strings, 'info' : preds})\n",
    "\n",
    "check_inference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format으로 맞춰주기\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['info'] = preds\n",
    "submission = test[['id', 'info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 생성할 때마다 파일명에 실험 내용과, 해당 날짜를 기재 -> 혼동 방지용\n",
    "prediction_path = './prediction/KoElecV3_withtitleSEP_len300_lr1e5_201211.csv'\n",
    "\n",
    "submission.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
